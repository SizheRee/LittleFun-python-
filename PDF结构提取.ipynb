{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用pdfminer3k获取所有的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.pdfparser import PDFParser,PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager,PDFPageInterpreter,PDFTextExtractionNotAllowed\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LTTextBoxHorizontal,LAParams,LTTextLineHorizontal,LTFigure,LTRect,LTLine,LTCurve\n",
    " \n",
    "#  文件对象\n",
    "pd_file = open(\"D:/Reaserch/packing/packing 问题综述/test.pdf\", \"rb\")\n",
    " \n",
    "#  pdf文件解析对象\n",
    "parser = PDFParser(pd_file)\n",
    " \n",
    "# print(parser)\n",
    "#  pdf文档对象\n",
    "document = PDFDocument()\n",
    "parser.set_document(document)\n",
    "document.set_parser(parser)\n",
    " \n",
    " \n",
    "#  初始化文档密码\n",
    "document.initialize()\n",
    "if document.is_extractable:\n",
    "    print(True)\n",
    "else:\n",
    "    raise PDFTextExtractionNotAllowed\n",
    "#  存储文档资源\n",
    "src = PDFResourceManager()\n",
    " \n",
    "#  设备对象\n",
    "device = PDFPageAggregator(src,laparams=LAParams())\n",
    " \n",
    "#  解释器对象\n",
    " \n",
    "inter = PDFPageInterpreter(src,device)\n",
    " \n",
    "pages = document.get_pages()\n",
    " \n",
    "for page in pages:\n",
    "    #print(page.contents)\n",
    "    inter.process_page(page)\n",
    "    layout = device.get_result()\n",
    "    for x in layout:\n",
    "        if isinstance(x, LTTextBoxHorizontal):\n",
    "            parse(str(x.get_text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(x):\n",
    "    if(x==\"References\"):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'D:\\\\Reaserch\\\\packing\\\\packing 问题综述\\x08ibliography.bib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b84846c299f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m Wang, Tianyi, et al. “Understanding graph sampling algorithms for social network analysis”, 31st International Conference on Distributed Computing Systems Workshops. IEEE, 2011.\"\"\"\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[0mmyFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:\\Reaserch\\packing\\packing 问题综述\\bibliography.bib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreference\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreferences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'D:\\\\Reaserch\\\\packing\\\\packing 问题综述\\x08ibliography.bib'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import re\n",
    "\n",
    "\n",
    "def make_query(reference, l=3):\n",
    "    f =  string.ascii_lowercase + ' '\n",
    "    q = reference\n",
    "    q = q.lower()\n",
    "    q = q.replace('-', ' ')\n",
    "    q = ''.join(c for c in q if c in f)\n",
    "    q = q.split(' ')\n",
    "    q = ' '.join(c for c in q if len(c) >= l)\n",
    "    return q\n",
    "\n",
    "def make_query_year(reference, l=3):\n",
    "    f =  string.ascii_lowercase + ' '\n",
    "    q = reference\n",
    "    id_re = re.compile(r'(\\d\\d\\d\\d)')\n",
    "    id = id_re.findall(q)\n",
    "    year = id[0] if len(id) > 0 else '('\n",
    "    q = q.split(year)\n",
    "    q = q[1:]\n",
    "    q = ''.join(q)\n",
    "    q = q.lower()\n",
    "    q = q.replace('-', ' ')\n",
    "    q = ''.join(c for c in q if c in f)\n",
    "    q = q.split(' ')\n",
    "    q = ' '.join(c for c in q if len(c) >= l)\n",
    "    if year != '(':\n",
    "        q = year + ' ' + q\n",
    "    return q\n",
    "\n",
    "def get_articles(query, count=1):\n",
    "    url = URL_SEARCH.format(q=query)\n",
    "    soup = http_get(url)\n",
    "    articles = []\n",
    "    for tag in soup.findAll(\"div\", { \"class\" : \"gs_r\" }):\n",
    "        a = tag.find('a', text='Cite', attrs={ \"class\" : \"gs_nph\" })\n",
    "        ident = re.search(r'gs_ocit\\(event,\\'(.*?)\\',', a.get('onclick', '')).group(1)\n",
    "        articles.append(ident)\n",
    "    if count > 0:\n",
    "        articles = articles[:count]\n",
    "    return articles\n",
    "\n",
    "def get_citations(ident, resolve=True):\n",
    "    url = URL_CITE.format(ident=ident)\n",
    "    soup = http_get(url)\n",
    "    citations = {}\n",
    "    for tag in soup.findAll('tr'):\n",
    "        citations[tag.th.text] = tag.td.text\n",
    "    for tag in soup.findAll('a', { \"class\": \"gs_citi\" }):\n",
    "        citations[tag.text] = tag.get('href')\n",
    "        if resolve:\n",
    "            citations[tag.text] = http_get(citations[tag.text]).text\n",
    "    return citations\n",
    "\n",
    "# Put references here:\n",
    "references = \"\"\"Ribeiro, Bruno, and Towsley. Don. “Estimating and sampling graphs with multidimensional random walks”, Proceedings of the 10th ACM SIGCOMM conference on Internet measurement. ACM, 2010.\n",
    "Wang, Tianyi, et al. “Understanding graph sampling algorithms for social network analysis”, 31st International Conference on Distributed Computing Systems Workshops. IEEE, 2011.\"\"\"\n",
    "\n",
    "myFile = open('D:\\Reaserch\\packing\\packing 问题综述\\bibliography.bib', 'w', encoding='utf-8')\n",
    "\n",
    "for reference in references.split('\\n'):\n",
    "    reference = reference.strip()\n",
    "    uprint (\"\\n---------------------------------------------------------\")\n",
    "    uprint (\"Doing reference:\", reference)\n",
    "    q = make_query(reference)\n",
    "    uprint (\"Query used:\", q)\n",
    "    r = get_articles(q)\n",
    "    if len(r) == 0:\n",
    "        q = make_query_year(reference)\n",
    "        r = get_articles(q)\n",
    "    if len(r) == 0:\n",
    "        uprint (\"Still no results -- skipping\")\n",
    "        continue\n",
    "    uprint (\"Result written\")\n",
    "    myFile.write('\\n\\n%%% '+ r[0] + '  '+ q +'\\n')\n",
    "    myFile.write(get_citations(r[0]).get('BibTeX'))\n",
    "\n",
    "myFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
